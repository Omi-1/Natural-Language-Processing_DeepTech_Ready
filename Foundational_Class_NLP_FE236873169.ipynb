{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRbkEHPTXy7X"
      },
      "source": [
        "# Week 4: Fundamentals of Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul_BDF1fX3no"
      },
      "source": [
        "**1. Define Natural Language Processing (NLP) in your own words.**:\n",
        "* Natural Language processing is a way of teaching the computer or machine to understand human language, and this goes beyound just understanding words in human languge but also understanding the content and sentiments in humnan language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiekpfMIX_KK"
      },
      "source": [
        "**2. List at least three real-world applications of NLP and explain their significance.**\n",
        "\n",
        "*   Building Chatbots - Chatbots provides a conversational way to interact with humans.\n",
        "*   Sentiment Analysis - Helps in understanding context and emotional tone in sentences\n",
        "*   Machine Translation - Allowing machines to understand multiple languages, allowing us preserve our languages digitally\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtLVZVS5XyBH"
      },
      "source": [
        "**3. Identify and explain two challenges that make NLP complex.**\n",
        "\n",
        "\n",
        "*   Ambiguity in Language - Words and Sentences have different meaning depending on the context e.g when dealing with metaphors i.e \"Time is a thief\", or words with multiple meaning e.g Bank i.e. financial institution, or i.e. River Bank\n",
        "*   Diversity in Human Language - Diffrent grammer and dialects that exist in human laguages makes it hard for machine to generalize languages, complicating NLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCtU0HCjhLA2"
      },
      "source": [
        "**4. Extract the following patterns using regex:**\n",
        "\n",
        "a) All email addresses from the text below:\n",
        "‚ÄúContact us at support@company.com or sales@business.org. For more, email info@service.net.‚Äù\n",
        "\n",
        "b) All words that end with \"ing\" from this sentence:\n",
        "‚ÄúNLP is amazing for cleaning and processing text while learning new\n",
        "techniques.‚Äù:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "pH0tP0sCM9kZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Extract email addresses\n",
        "text_emails = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net.\"\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "emails = re.findall(email_pattern, text_emails)\n",
        "print(\"Email addresses:\", emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzwZFI0ALWkP",
        "outputId": "d7c21b22-7884-49c8-fd32-b6152bf5df43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email addresses: ['support@company.com', 'sales@business.org', 'info@service.net']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Extract words ending with \"ing\"\n",
        "text_ing = \"NLP is amazing for cleaning and processing text while learning new techniques.\"\n",
        "ing_pattern = r'\\b\\w+ing\\b'\n",
        "ing_words = re.findall(ing_pattern, text_ing)\n",
        "print(\"Words ending with 'ing':\", ing_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmIlYr6vLWn6",
        "outputId": "994665f6-6fe1-4e03-d0ae-7462eb9659c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words ending with 'ing': ['amazing', 'cleaning', 'processing', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to clean the following text by:\n",
        "‚ÄúNLP makes AI smarter! But, sometimes, it‚Äôs challenging‚Ä¶ Don‚Äôt you agree?‚Äù\n",
        "* a) Removing all punctuation.\n",
        "* b) Converting it to lowercase.\n",
        "* c) Splitting it into words."
      ],
      "metadata": {
        "id": "sXnREW-8M53t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Original text\n",
        "text = \"NLP makes AI smarter! But, sometimes, it‚Äôs challenging‚Ä¶ Don‚Äôt you agree?\"\n",
        "\n",
        "# a) Remove all punctuation\n",
        "text_no_punct = text.translate(str.maketrans('', '', string.punctuation + \"‚Ä¶‚Äô\"))\n",
        "\n",
        "# b) Convert to lowercase\n",
        "text_lower = text_no_punct.lower()\n",
        "\n",
        "# c) Split into words\n",
        "words = text_lower.split()\n",
        "\n",
        "# Display results\n",
        "print(\"Cleaned words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxk6BN9JLWql",
        "outputId": "7f070a34-7b26-4ebf-ef96-ff7ed07f9af9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned words: ['nlp', 'makes', 'ai', 'smarter', 'but', 'sometimes', 'its', 'challenging', 'dont', 'you', 'agree']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applied Learning Assignments 2:\n",
        "1. Text Cleaning Task\n",
        "Apply text cleaning techniques to preprocess the following text:\n",
        "\"OMG!! NLP is soooo coool ü§©...!!! It costs $1000. Learn it now at https://3mtt.com üòé.\"\n",
        "Refer to the course slide for more information\n",
        "\n",
        "2. Tokenization Task\n",
        "Perform both word-level and sentence-level tokenization on the given\n",
        "text.\n",
        "\"Tokenization is the first step in NLP. It splits text into smaller pieces for\n",
        "analysis.\"\n",
        "* Use NLTK to perform word tokenization.\n",
        "* Use NLTK to perform sentence tokenization"
      ],
      "metadata": {
        "id": "ywOZA838Nt2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original text\n",
        "text = \"OMG!! NLP is soooo coool ü§©...!!! It costs $1000. Learn it now at https://3mtt.com üòé.\"\n",
        "\n",
        "# Step 1: Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Step 2: Remove URLs\n",
        "text = re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "# Step 3: Remove emojis (basic emoji pattern)\n",
        "text = re.sub(r'[^\\w\\s.,!?$]', '', text)\n",
        "\n",
        "# Step 4: Remove punctuation\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Step 5: Normalize elongated words (e.g., sooo -> so)\n",
        "text = re.sub(r'([aeiou])\\1{2,}', r'\\1', text)\n",
        "\n",
        "# Step 6: Remove extra whitespace\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Final cleaned text\n",
        "print(\"Cleaned Text:\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9G_PQBrNzSm",
        "outputId": "620f9c73-0f4c-474f-e370-db58e02e11ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text: omg nlp is so col it costs 1000 learn it now at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stemming and Lemmatization Task\n",
        "Apply stemming and lemmatization techniques to a list of words:\n",
        "[\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n",
        "* Use Porter Stemmer to perform stemming on the words.\n",
        "* Use spaCy to perform lemmatization on the same words.\n"
      ],
      "metadata": {
        "id": "-LHQCkG0PF_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Porter Stemmer (from nltk)\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n",
        "\n",
        "stemmed_words = [ps.stem(word) for word in words]\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "\n",
        "\n",
        "# 2. Lemmatization using spaCy\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the list of words as a single string\n",
        "doc = nlp(\" \".join(words))\n",
        "\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CDMPI-nPGu8",
        "outputId": "ad23e654-6527-4f38-a0cd-72340198a6fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['run', 'fli', 'studi', 'easili', 'studi', 'better']\n",
            "Lemmatized words: ['run', 'fly', 'study', 'easily', 'study', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applied Learning Assignments 3:\n",
        "1. Define a vocabulary of at least 5 unique words. Write Python code to\n",
        "generate one-hot encoded vectors for your vocabulary.\n",
        "2. Use the following sentences as your dataset:\n",
        "‚Äì Write Python code to generate a Bag of Words representation for the\n",
        "dataset using CountVectorizer.\n",
        "‚Äì Write Python code to compute the TF-IDF representation using\n",
        "TfidfVectorizer.\n",
        "Week 4: Fundamentals of Natural Language Processing\n",
        "* ‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù\n",
        "* ‚ÄúThe dog sleeps in the kernel‚Äù\n",
        "Applied Learning Assignments 3:\n",
        "3. Create a small dataset of at least 3 sentences related to animals.\n",
        "Example: \"The cat meows. The dog barks. The bird sings.\"\n",
        "‚Äì Write Python code to:\n",
        "* Train a Word2Vec model using gensim.\n",
        "* Retrieve the embedding for the word \"dog\".\n",
        "4. Load the pretrained GloVe model (glove-wiki-gigaword-50) using gensim.\n",
        "‚Äì Write Python code to:\n",
        "* Retrieve the embedding for the word \"king\".\n",
        "* Find the 5 most similar words to \"king\"."
      ],
      "metadata": {
        "id": "vMeOfcbpPVK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Define a small vocabulary\n",
        "vocab = ['cat', 'dog', 'bird', 'lion', 'tiger']\n",
        "vocab = np.array(vocab).reshape(-1, 1)\n",
        "\n",
        "# Initialize and fit the encoder\n",
        "encoder = OneHotEncoder()\n",
        "one_hot = encoder.fit_transform(vocab)\n",
        "\n",
        "# Print results\n",
        "print(one_hot.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOgc6yfFPUI1",
        "outputId": "7a8b5e05-5802-4270-e9aa-88ec38c82836"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "dataset = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog sleeps in the kernel\"\n",
        "]\n",
        "\n",
        "# Initialize and fit CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform(dataset)\n",
        "\n",
        "# Show vocabulary and BoW matrix\n",
        "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
        "print(\"BoW Matrix:\\n\", bow.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4i-3A36c4o8",
        "outputId": "f4d8b481-2037-43b2-fe65-765f742cab9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'the': 10, 'quick': 8, 'brown': 0, 'fox': 2, 'jumps': 4, 'over': 7, 'lazy': 6, 'dog': 1, 'sleeps': 9, 'in': 3, 'kernel': 5}\n",
            "BoW Matrix:\n",
            " [[1 1 1 0 1 0 1 1 1 0 2]\n",
            " [0 1 0 1 0 1 0 0 0 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize and fit TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(dataset)\n",
        "\n",
        "# Show vocabulary and TF-IDF matrix\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.vocabulary_)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3lHvtEHflaQ",
        "outputId": "72ebc776-5df4-4967-8373-c4072394e2f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'the': 10, 'quick': 8, 'brown': 0, 'fox': 2, 'jumps': 4, 'over': 7, 'lazy': 6, 'dog': 1, 'sleeps': 9, 'in': 3, 'kernel': 5}\n",
            "TF-IDF Matrix:\n",
            " [[0.342369   0.24359836 0.342369   0.         0.342369   0.\n",
            "  0.342369   0.342369   0.342369   0.         0.48719673]\n",
            " [0.         0.30253071 0.         0.42519636 0.         0.42519636\n",
            "  0.         0.         0.         0.42519636 0.60506143]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [\n",
        "    [\"the\", \"cat\", \"meows\"],\n",
        "    [\"the\", \"dog\", \"barks\"],\n",
        "    [\"the\", \"bird\", \"sings\"]\n",
        "]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=1)\n",
        "\n",
        "# Get embedding for 'dog'\n",
        "print(\"Embedding for 'dog':\\n\", model.wv['dog'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELZvhVJVfszQ",
        "outputId": "5adec111-3ddd-47c6-b884-fdc0a7d40f80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'dog':\n",
            " [ 0.00018913  0.00615464 -0.01362529 -0.00275093  0.01533716  0.01469282\n",
            " -0.00734659  0.0052854  -0.01663426  0.01241097 -0.00927464 -0.00632821\n",
            "  0.01862271  0.00174677  0.01498141 -0.01214813  0.01032101  0.01984565\n",
            " -0.01691478 -0.01027138 -0.01412967 -0.0097253  -0.00755713 -0.0170724\n",
            "  0.01591121 -0.00968788  0.01684723  0.01052514 -0.01310005  0.00791574\n",
            "  0.0109403  -0.01485307 -0.01481144 -0.00495046 -0.01725145 -0.00316314\n",
            " -0.00080687  0.00659937  0.00288376 -0.00176284 -0.01118812  0.00346073\n",
            " -0.00179474  0.01358738  0.00794718  0.00905894  0.00286861 -0.00539971\n",
            " -0.00873363 -0.00206415]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}